{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名词解释\n",
    "- POS Part-Of-Speech 词性标注，动词，名词，形容词 https://en.wikipedia.org/wiki/Part-of-speech_tagging\n",
    "\n",
    "### 词的表达\n",
    "- 在传统的NLP中，用词典One hot编码，`[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] `。\n",
    "- 传统词向量的问题\n",
    "    - 占内存随着词典增大而增大\n",
    "    - 维度非常大，耗内存\n",
    "    - 系数特征会使得模型不鲁班\n",
    "   \n",
    "  因此需要对词向量降维。\n",
    "- SVD降维   \n",
    "  出现高级语义模式。\n",
    "  论文：An Improved Model of Seman3c Similarity Based on Lexical CoKOccurrence Rohde et al. 2005    \n",
    "  问题：\n",
    "    - 数据量大的时候，计算量太大。\n",
    "    - 没有考虑新词\n",
    "- 直接学习低维词向量  \n",
    "  - Learning representa3ons by backKpropaga3ng errors. (Rumelhart et al., 1986) \n",
    "  - A neural probabilis3c language model (Bengio et al., 2003)   \n",
    "  - NLP from Scratch (Collobert & Weston, 2008) \n",
    "  - A recent and even simpler model:  word2vec (Mikolov et al. 2013) ! intro now\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简化的word2vec模型\n",
    "代价函数    \n",
    "$$\n",
    "J(\\theta)= \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c\\le j\\le c, j \\ne 0} \\log p(w_{t+j}|w_t)\n",
    "$$  \n",
    "\n",
    "其中概率定义为\n",
    "$$\n",
    "p(w_O|w_I) = \\frac{\\exp(\\hat{v}_{w_O}^T v_{w_I})}{\\sum_{w=1}^W \\exp(\\hat{v}_{w}^T v_{w_I})}\n",
    "$$\n",
    "\n",
    "也就是每个词对应两个向量 $v$和$\\hat{v}$。\n",
    "\n",
    "优化算法采用梯度下降， 或随机梯度下降。随机梯度下降时，每次更新的梯度是稀疏的。\n",
    "\n",
    "最大计算量在分母上，每一次迭代需要的计算量与词典大小$W$成正比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语料\n",
    "Google word2vec 语料 http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "### 资源\n",
    "- gensim word2vec \n",
    "- python-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "f = open('data/text8')\n",
    "corpus = f.read(1000000)\n",
    "f.close()\n",
    "word_list = corpus.split(' ')[1:-1]\n",
    "word_dict = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob(theta, n_word, n_features):\n",
    "    L = theta[:n_word*n_features].reshape((n_features, n_word))\n",
    "    LL = theta[n_word*n_features:].reshape((n_features, n_word))\n",
    "    word_dot = np.exp(np.dot(LL.T, L))\n",
    "    B = np.sum(word_dot, axis=0).reshape((1,n_word))\n",
    "    #print word_dot.shape,B.shape\n",
    "    p = word_dot / B\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.8662970023\n"
     ]
    }
   ],
   "source": [
    "n_word = 3\n",
    "n_features = 100\n",
    "theta = np.random.rand(2*n_word*n_features)\n",
    "#theta[:n_features]=0\n",
    "p = prob(theta, n_word, n_features)\n",
    "word = np.random.randint(0,3,1000)\n",
    "c = 3\n",
    "ker = np.array([1]*c + [0] + [1]*c)\n",
    "\n",
    "from scipy.signal import convolve2d,convolve\n",
    "convp = convolve(np.log(p[word,word]), ker, 'valid')\n",
    "J = convp.mean()\n",
    "print J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充numpy的nditer\n",
    "The iterator object nditer, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion. This page introduces some basic ways to use the object for computations on arrays in Python, then concludes with how one can accelerate the inner loop in Cython. Since the Python exposure of nditer is a relatively straightforward mapping of the C array iterator API, these ideas will also provide help working with array iteration from C or C++.\n",
    "\n",
    "简单来说nditer会帮你加速。参考[页面](http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html#arrays-nditer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3)\n",
    "for x in np.nditer(a):\n",
    "    print x,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nditer只会按照内存存储的实际顺序遍历，不会按照行先或者列先的顺序访问的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a.T):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 4 2 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a.T.copy(order='C')):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 控制nditer的迭代顺序\n",
    "The default, having the behavior described above, is order=’K’ to keep the existing order. This can be overridden with order=’C’ for C order and order=’F’ for Fortran order.\n",
    "\n",
    "Fortran是列主序，C是行主序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 4 2 5\n",
      "0 1 2 3 4 5\n"
     ]
    }
   ],
   "source": [
    "for x in np.nditer(a, order='F'):  \n",
    "    print x,\n",
    "print \n",
    "for x in np.nditer(a, order='C'):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram model\n",
    "- 论文：Distributed Representations of Words and Phrases and their Compositionality, Mikolov, 2013\n",
    "- 基本思想：对每一个上下文单词与中心单词，训练一个二元逻辑回归，中心单词随机负采样。\n",
    "- 优化目标函数\n",
    "$$\n",
    "F(w_i|r) = \\log \\sigma(w_i^T r) + \\sum_{j=1}^K E_{w_j \\sim p_n(w)} \\log(\\sigma(-w_j^T r))\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{skip-gram}(w_{i-c},\\dots , w_{i+c}) = \\sum_{-c\\le j \\le c, j\\ne 0} F(w_{i+j}|r)\n",
    "$$\n",
    "\n",
    "### CBOW\n",
    "skip-gram model是用中间的词预测上下文，而CBOW用上下文预测中间词。\n",
    "$$ r_i =  \\sum_{-c\\le j \\le c, j\\ne 0} w_{i+j}\n",
    "$$\n",
    "$$ J_{CBOW}(w_{i-c},\\dots , w_{i+c}) =  F(w_{i}|r_i)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
